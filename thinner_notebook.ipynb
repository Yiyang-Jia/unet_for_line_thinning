{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4544281c-e817-4b81-9915-853532da7413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import os\n",
    "import unet_model\n",
    "\n",
    "\n",
    "# transform images to a standard size\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = unet_model.LineDataset('thick_lines_synthetic/', 'thin_lines_synthetic/', transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for both splits\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766f39b-7f5e-40f7-9927-13f7cb5cca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, and optimizer\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "model = unet_model.UNet().to(device)\n",
    "model_path = 'unet_line_thinning_model.pth' \n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "train_loss_list = [100]\n",
    "val_loss_list = [100]\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    with torch.no_grad():\n",
    "            val_losses=[]\n",
    "            for (val_data, val_target) in val_loader:\n",
    "                val_data, val_target = val_data.to(device), val_target.to(device)\n",
    "                val_output = model(val_data)\n",
    "                val_losses.append(criterion(val_output, val_target).item())\n",
    "            val_loss = np.mean(val_losses)\n",
    "            val_loss_list[0] = val_loss\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        train_loss = criterion(output, target)\n",
    "        train_loss_list.append(train_loss.item())\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_losses=[]\n",
    "            for (val_data, val_target) in val_loader:\n",
    "                val_data, val_target = val_data.to(device), val_target.to(device)\n",
    "                val_output = model(val_data)\n",
    "                val_losses.append(criterion(val_output, val_target).item())\n",
    "            val_loss = np.mean(val_losses)\n",
    "        \n",
    "        if val_loss < min(val_loss_list):\n",
    "           torch.save(model.state_dict(), model_path)\n",
    "        val_loss_list.append(val_loss )\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {train_loss.item():.6f} ' \n",
    "                  f'  validation loss: {val_loss:.6f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2f3402-bdae-42f7-874c-d32e7416cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_model(model_path):\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    model = unet_model.UNet().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    return model\n",
    "\n",
    "# Function to preprocess the input image\n",
    "def preprocess_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Function to perform inference\n",
    "def predict(model, image):\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "    return output\n",
    "\n",
    "# Function to post-process the output\n",
    "def postprocess_output(output):\n",
    "    output = torch.sigmoid(output)  # Apply sigmoid to get probability map\n",
    "    output = output.squeeze().cpu().numpy()  # Remove batch dimension and convert to numpy array\n",
    "    return (output > 0.95).astype(np.uint8) * 255  # Threshold and convert to binary image\n",
    "\n",
    "\n",
    " # Path to your saved model\n",
    "image_path = 'nature_imag_cropped.png'  # Path to a test image\n",
    "#image_path = 'thick_lines/run_001.png'  # Path to a test image\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model_path = 'unet_line_thinning_model.pth'  #shoule be the same as defined in the training block\n",
    "model = load_model(model_path)\n",
    "\n",
    "# Preprocess the image\n",
    "input_image = preprocess_image(image_path)\n",
    "\n",
    "\n",
    "# Perform inference\n",
    "output = predict(model, input_image)\n",
    "output.shape\n",
    "plt.imshow( output.squeeze(0).squeeze(0),cmap='gray')\n",
    "\n",
    "# Post-process the output\n",
    "result = postprocess_output(output)\n",
    "# # Save or display the result\n",
    "# result_image = Image.fromarray(result)\n",
    "# plt.imshow(result_image)\n",
    "\n",
    "# result_image.save('thinned_line_result.png')\n",
    "# # print(\"Thinned line image saved as 'thinned_line_result.png'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cba706-8bff-4a58-bb81-6700e04baae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_image.squeeze(0).squeeze(0),cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b94a02-5c98-41fc-9df9-2ef8415b1551",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(result,cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640a5255-dd83-40b2-a18a-6bba7fcb7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_drawn_line =  preprocess_image('hand_drawn_cropped.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9477a2-7abf-4719-baa8-85ceb770eb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(hand_drawn_line.squeeze(0).squeeze(0),cmap='gray')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae2ed0-4831-423e-8c2a-a8e7ad85a731",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 6))\n",
    "im1 = input_image.squeeze(0).squeeze(0)\n",
    "im2 = result\n",
    "im3 = hand_drawn_line.squeeze(0).squeeze(0)\n",
    "# Display the first image\n",
    "im1_display = ax1.imshow(im1, cmap='gray' if im1.ndim == 2 else None)\n",
    "#ax1.axis('off')  # Hide axes\n",
    "\n",
    "# Display the second image\n",
    "im2_display = ax2.imshow(im2, cmap='gray' if im2.ndim == 2 else None)\n",
    "#ax2.axis('off')  # Hide axes\n",
    "\n",
    "im2_display = ax3.imshow(im3, cmap='gray' if im2.ndim == 2 else None)\n",
    "\n",
    "# Add colorbars if the images are 2D (likely grayscale)\n",
    "# if im1.ndim == 2:\n",
    "#     fig.colorbar(im1_display, ax=ax1)\n",
    "# if im2.ndim == 2:\n",
    "#     fig.colorbar(im2_display, ax=ax2)\n",
    "\n",
    "# Adjust the layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f314cf-356e-487e-b122-e1b4fb68c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def save_three_arrays_side_by_side(im1, im2, im3, save_path, titles=None, fig_size=(18, 6), dpi=300):\n",
    "    # Create a figure with three subplots side by side\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=fig_size)\n",
    "    \n",
    "    # Function to display a single image\n",
    "    def display_image(ax, im, title):\n",
    "        im_display = ax.imshow(im, cmap='gray' if im.ndim == 2 else None)\n",
    "        #ax.axis('off')  # Hide axes\n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "        #if im.ndim == 2:\n",
    "            #fig.colorbar(im_display, ax=ax)\n",
    "    \n",
    "    # Display the three images\n",
    "    display_image(ax1, im1, titles[0] if titles else None)\n",
    "    display_image(ax2, im2, titles[1] if titles else None)\n",
    "    display_image(ax3, im3, titles[2] if titles else None)\n",
    "    \n",
    "    # Adjust the layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(save_path, dpi=dpi, bbox_inches='tight')\n",
    "    \n",
    "    # Close the figure to free up memory\n",
    "    plt.close(fig)\n",
    "    \n",
    "    print(f\"Side-by-side image saved to {save_path}\")\n",
    "\n",
    "# Example usage\n",
    "# im1, im2, and im3 are your NumPy arrays\n",
    "save_three_arrays_side_by_side(im1, im3, im2, 'three_images_comparison.png', titles=['original','ground truth',\n",
    "                                                                                     'unet-processed' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7b3c98-ee21-4c6a-864c-0d08823c6e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "im4=output.squeeze(0).squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf6d8a2-250d-48d9-aa32-60b9a22ce19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_three_arrays_side_by_side(im1, im3, im4, 'three_images_comparison_unfiltered.png', titles=['original','ground truth',\n",
    "                                                                                     'unet-processed' ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
